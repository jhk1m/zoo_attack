{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 2, 506)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 506, 2)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 506, 64)      704         permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 506, 64)      256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 506, 64)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 64)        0           global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1, 4)         256         reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1, 64)        256         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 506, 64)      0           activation_7[0][0]               \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 506, 128)     32896       multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 506, 128)     512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 506, 128)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 128)       0           global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 8)         1024        reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 128)       1024        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 506, 128)     0           activation_8[0][0]               \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 506, 64)      24640       multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, 2, 506)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 506, 64)      256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 8)            16480       masking_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 506, 64)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8)            0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 64)           0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 72)           0           dropout_3[0][0]                  \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 7)            511         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 78,815\n",
      "Trainable params: 78,303\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "Loading train / test dataset :  ../cut_data/ ../cut_data/\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  1847 Number of test samples :  793\n",
      "Number of classes :  7\n",
      "Sequence length :  506\n",
      "Class weights :  [2.0776153  2.09410431 1.77085331 0.78763326 0.47456321 1.12759463\n",
      " 0.82455357]\n",
      "Train on 1847 samples, validate on 793 samples\n",
      "Epoch 1/30\n",
      " - 11s - loss: 1.6347 - accuracy: 0.3785 - val_loss: 1.6302 - val_accuracy: 0.3291\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.63472, saving model to ./weights\n",
      "Epoch 2/30\n",
      " - 8s - loss: 1.2706 - accuracy: 0.5425 - val_loss: 1.5838 - val_accuracy: 0.3455\n",
      "\n",
      "Epoch 00002: loss improved from 1.63472 to 1.27060, saving model to ./weights\n",
      "Epoch 3/30\n",
      " - 9s - loss: 1.0255 - accuracy: 0.6800 - val_loss: 1.2392 - val_accuracy: 0.4968\n",
      "\n",
      "Epoch 00003: loss improved from 1.27060 to 1.02554, saving model to ./weights\n",
      "Epoch 4/30\n",
      " - 10s - loss: 0.8997 - accuracy: 0.6995 - val_loss: 1.0795 - val_accuracy: 0.6028\n",
      "\n",
      "Epoch 00004: loss improved from 1.02554 to 0.89968, saving model to ./weights\n",
      "Epoch 5/30\n",
      " - 9s - loss: 0.8053 - accuracy: 0.7228 - val_loss: 1.0651 - val_accuracy: 0.5990\n",
      "\n",
      "Epoch 00005: loss improved from 0.89968 to 0.80530, saving model to ./weights\n",
      "Epoch 6/30\n",
      " - 9s - loss: 0.7368 - accuracy: 0.7520 - val_loss: 0.8227 - val_accuracy: 0.7314\n",
      "\n",
      "Epoch 00006: loss improved from 0.80530 to 0.73680, saving model to ./weights\n",
      "Epoch 7/30\n",
      " - 9s - loss: 0.7011 - accuracy: 0.7553 - val_loss: 0.7550 - val_accuracy: 0.7163\n",
      "\n",
      "Epoch 00007: loss improved from 0.73680 to 0.70111, saving model to ./weights\n",
      "Epoch 8/30\n",
      " - 9s - loss: 0.6245 - accuracy: 0.7775 - val_loss: 0.8195 - val_accuracy: 0.6721\n",
      "\n",
      "Epoch 00008: loss improved from 0.70111 to 0.62454, saving model to ./weights\n",
      "Epoch 9/30\n",
      " - 8s - loss: 0.6145 - accuracy: 0.7861 - val_loss: 0.9164 - val_accuracy: 0.6280\n",
      "\n",
      "Epoch 00009: loss improved from 0.62454 to 0.61450, saving model to ./weights\n",
      "Epoch 10/30\n",
      " - 8s - loss: 0.5760 - accuracy: 0.7959 - val_loss: 1.0465 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00010: loss improved from 0.61450 to 0.57601, saving model to ./weights\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.5557 - accuracy: 0.8110 - val_loss: 0.7725 - val_accuracy: 0.7112\n",
      "\n",
      "Epoch 00011: loss improved from 0.57601 to 0.55574, saving model to ./weights\n",
      "Epoch 12/30\n",
      " - 8s - loss: 0.5282 - accuracy: 0.8181 - val_loss: 0.7241 - val_accuracy: 0.7238\n",
      "\n",
      "Epoch 00012: loss improved from 0.55574 to 0.52816, saving model to ./weights\n",
      "Epoch 13/30\n",
      " - 8s - loss: 0.5090 - accuracy: 0.8273 - val_loss: 0.5717 - val_accuracy: 0.7995\n",
      "\n",
      "Epoch 00013: loss improved from 0.52816 to 0.50901, saving model to ./weights\n",
      "Epoch 14/30\n",
      " - 8s - loss: 0.5242 - accuracy: 0.8175 - val_loss: 0.5615 - val_accuracy: 0.7831\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.50901\n",
      "Epoch 15/30\n",
      " - 9s - loss: 0.4662 - accuracy: 0.8462 - val_loss: 0.9058 - val_accuracy: 0.6494\n",
      "\n",
      "Epoch 00015: loss improved from 0.50901 to 0.46622, saving model to ./weights\n",
      "Epoch 16/30\n",
      " - 9s - loss: 0.4696 - accuracy: 0.8370 - val_loss: 0.5278 - val_accuracy: 0.8058\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.46622\n",
      "Epoch 17/30\n",
      " - 10s - loss: 0.4927 - accuracy: 0.8257 - val_loss: 2.0399 - val_accuracy: 0.4149\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.46622\n",
      "Epoch 18/30\n",
      " - 9s - loss: 0.4805 - accuracy: 0.8327 - val_loss: 0.7344 - val_accuracy: 0.7377\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.46622\n",
      "Epoch 19/30\n",
      " - 10s - loss: 0.4414 - accuracy: 0.8538 - val_loss: 0.7464 - val_accuracy: 0.7327\n",
      "\n",
      "Epoch 00019: loss improved from 0.46622 to 0.44135, saving model to ./weights\n",
      "Epoch 20/30\n",
      " - 10s - loss: 0.4196 - accuracy: 0.8587 - val_loss: 0.9676 - val_accuracy: 0.5939\n",
      "\n",
      "Epoch 00020: loss improved from 0.44135 to 0.41961, saving model to ./weights\n",
      "Epoch 21/30\n",
      " - 10s - loss: 0.4328 - accuracy: 0.8473 - val_loss: 1.0432 - val_accuracy: 0.6015\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.41961\n",
      "Epoch 22/30\n",
      " - 10s - loss: 0.4046 - accuracy: 0.8571 - val_loss: 1.7791 - val_accuracy: 0.5574\n",
      "\n",
      "Epoch 00022: loss improved from 0.41961 to 0.40461, saving model to ./weights\n",
      "Epoch 23/30\n",
      " - 10s - loss: 0.4280 - accuracy: 0.8473 - val_loss: 0.5708 - val_accuracy: 0.8033\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.40461\n",
      "Epoch 24/30\n",
      " - 10s - loss: 0.3975 - accuracy: 0.8636 - val_loss: 2.3684 - val_accuracy: 0.4439\n",
      "\n",
      "Epoch 00024: loss improved from 0.40461 to 0.39750, saving model to ./weights\n",
      "Epoch 25/30\n",
      " - 10s - loss: 0.3910 - accuracy: 0.8690 - val_loss: 0.5943 - val_accuracy: 0.8247\n",
      "\n",
      "Epoch 00025: loss improved from 0.39750 to 0.39105, saving model to ./weights\n",
      "Epoch 26/30\n",
      " - 9s - loss: 0.3936 - accuracy: 0.8663 - val_loss: 0.4469 - val_accuracy: 0.8335\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.39105\n",
      "Epoch 27/30\n",
      " - 9s - loss: 0.3885 - accuracy: 0.8576 - val_loss: 0.4434 - val_accuracy: 0.8348\n",
      "\n",
      "Epoch 00027: loss improved from 0.39105 to 0.38852, saving model to ./weights\n",
      "Epoch 28/30\n",
      " - 9s - loss: 0.3912 - accuracy: 0.8722 - val_loss: 4.3011 - val_accuracy: 0.3670\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.38852\n",
      "Epoch 29/30\n",
      " - 9s - loss: 0.3597 - accuracy: 0.8744 - val_loss: 4.2021 - val_accuracy: 0.3241\n",
      "\n",
      "Epoch 00029: loss improved from 0.38852 to 0.35970, saving model to ./weights\n",
      "Epoch 30/30\n",
      " - 9s - loss: 0.3508 - accuracy: 0.8771 - val_loss: 7.3597 - val_accuracy: 0.2863\n",
      "\n",
      "Epoch 00030: loss improved from 0.35970 to 0.35080, saving model to ./weights\n",
      "Loading train / test dataset :  ../cut_data/ ../cut_data/\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  1847 Number of test samples :  793\n",
      "Number of classes :  7\n",
      "Sequence length :  506\n",
      "\n",
      "Evaluating : \n",
      "793/793 [==============================] - 2s 3ms/step\n",
      "\n",
      "Final Accuracy :  0.28625473380088806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.28625473380088806, 7.359690446120042)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "from utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\n",
    "from utils.keras_utils import train_model, evaluate_model, set_trainable\n",
    "from utils.layer_utils import AttentionLSTM\n",
    "\n",
    "DATASET_INDEX = 0\n",
    "\n",
    "MAX_TIMESTEPS = MAX_TIMESTEPS_LIST[DATASET_INDEX]\n",
    "MAX_NB_VARIABLES = MAX_NB_VARIABLES[DATASET_INDEX]\n",
    "NB_CLASS = NB_CLASSES_LIST[DATASET_INDEX]\n",
    "\n",
    "TRAINABLE = True\n",
    "\n",
    "def generate_model():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 4, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se\n",
    "\n",
    "model = generate_model()\n",
    "\n",
    "train_model(model, DATASET_INDEX, dataset_prefix='vehicle_data', epochs=30, batch_size=16)\n",
    "\n",
    "evaluate_model(model, DATASET_INDEX, dataset_prefix='vehicle_data', batch_size=16)\n",
    "\n",
    "model.save('/Users/yaofan29597/Desktop/UVA/course/Cloud Computing/project/code/MLSTM_FCN/test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 7., 7.], dtype=float32)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "\n",
    "tlab = tf.constant(np.ones(7), dtype=tf.float32)\n",
    "opt = tf.placeholder(tf.float32,[3, 7])\n",
    "real = tf.reduce_sum((tlab)*opt,1)\n",
    "ll = tf.maximum(0.0, real)\n",
    "\n",
    "sess.run(ll, feed_dict={opt:np.ones((3,7))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train / test dataset :  ../cut_data/ ../cut_data/\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  1847 Number of test samples :  793\n",
      "Number of classes :  7\n",
      "Sequence length :  506\n",
      "793/793 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5680130276673979"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7856242060661316"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from utils.generic_utils import load_dataset_at\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils.constants import MAX_NB_VARIABLES, MAX_TIMESTEPS_LIST\n",
    "\n",
    "ckpt_path = '/Users/yaofan29597/Desktop/UVA/course/Cloud Computing/project/code/MLSTM_FCN/test_model.h5'\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(ckpt_path)\n",
    "\n",
    "X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(0,fold_index=None,normalize_timeseries=False) \n",
    "X_test = pad_sequences(X_test, maxlen=MAX_NB_VARIABLES[0], padding='post', truncating='post')\n",
    "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test[:], y_test[:], batch_size=16)\n",
    "\n",
    "loss\n",
    "accuracy\n",
    "\n",
    "\n",
    "tlab = tf.constant(np.zeros(7), dtype=tf.float32)\n",
    "opt = tf.constant(np.zeros(3,7), dtype=tf.float32)\n",
    "real = tf.reduce_sum((tlab)*opt,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaofan29597/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Three Running Mode: whitebox/fake_blackbox/blackbox. \\n    Select one of these 3 options as True to proceed.\\n'"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Construct tf-Graph\\n'"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-521-9d2bec2e5317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# initialize variables and load target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1235\u001b[0m                              ' elements.')\n\u001b[1;32m   1236\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2958\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m     \"\"\"\n\u001b[0;32m-> 2960\u001b[0;31m     \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3257\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3259\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "'''\n",
    "    Three Running Mode: whitebox/fake_blackbox/blackbox. \n",
    "    Select one of these 3 options as True to proceed.\n",
    "'''\n",
    "# whitebox, fake_blackbox, blackbox = True, False, False\n",
    "whitebox, fake_blackbox, blackbox = False, True, False\n",
    "# whitebox, fake_blackbox, blackbox = False, False, True\n",
    "if whitebox:\n",
    "    use_train_op = True\n",
    "if fake_blackbox:\n",
    "    use_train_op, use_grad_op = False, True\n",
    "if blackbox:\n",
    "    use_train_op, use_grad_op = False, False\n",
    "\n",
    "    \n",
    "'''\n",
    "    Construct tf-Graph\n",
    "''' \n",
    "CONST_LAMBDA = 1000\n",
    "SUCCESS_ATTACK_PROB_THRES = 0.2\n",
    "x = tf.placeholder(tf.float32,[None, 2, 506])\n",
    "y = tf.placeholder(tf.float32,[None, 7])\n",
    "\n",
    "# In whitebox attack, Var adv is updated thru train_op, while in blackbox attack, adv is updated manually. \n",
    "with tf.name_scope('attack'):\n",
    "    if whitebox:\n",
    "        adv = tf.Variable(tf.zeros([1, 2, 506]), name = \"adv_pert\")\n",
    "    else:\n",
    "        adv = tf.placeholder(tf.float32, [None, 2, 506])\n",
    "\n",
    "# specify trainable variable\n",
    "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "attack_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='attack')\n",
    "trainable_vars = tf.trainable_variables()\n",
    "for var in all_vars:\n",
    "    if var not in attack_vars:\n",
    "        trainable_vars.remove(var)\n",
    "\n",
    "# compute loss\n",
    "new_x = adv + x\n",
    "output_x = model(x)\n",
    "output = model(new_x)\n",
    "\n",
    "l2dist = tf.reduce_sum(tf.square(adv), [1,2])\n",
    "\n",
    "real = tf.reduce_sum(y * output, 1)\n",
    "fake = tf.reduce_max((1 - y) * output, 1)\n",
    "    \n",
    "loss1 = CONST_LAMBDA * tf.maximum(-SUCCESS_ATTACK_PROB_THRES, real - fake)\n",
    "loss2 = l2dist\n",
    "loss_batch = loss1 + loss2\n",
    "loss = tf.reduce_sum(loss_batch) # sum over all the batch samples\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "\n",
    "# replace train_op with manual designed grad_op\n",
    "if use_train_op:\n",
    "    train = optimizer.minimize(loss, var_list=trainable_vars)\n",
    "if use_grad_op:\n",
    "    grad_op = tf.gradients(loss, adv)\n",
    "\n",
    "# initialize variables and load target model\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model.load_weights(ckpt_path)\n",
    "\n",
    "\n",
    "'''\n",
    "    Perform attack\n",
    "'''\n",
    "\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    \n",
    "    print(\"start attacking target\", i, \"...\")\n",
    "    \n",
    "    mt = 0               # accumulator m_t in Adam\n",
    "    vt = 0               # accumulator v_t in Adam\n",
    "    beta1=0.9            # parameter beta_1 in Adam\n",
    "    beta2=0.999          # parameter beta_2 in Adam\n",
    "    learning_rate = 1e-1 # learning rate\n",
    "    epsilon = 1e-8       # parameter epsilon in Adam\n",
    "    h = 0.0001           # discretization constant when estimating numerical gradient\n",
    "    batch_size = 1       # batch size\n",
    "    MAX_ITE = 1000       # maximum number of iterations\n",
    "\n",
    "\n",
    "    real_adv = np.zeros([1, 2, 506])   # initial adversarial perturbation, the trainable variable\n",
    "    X = X_test[i:i+1]           # target sample X\n",
    "    Y = y_test[i:i+1]           # target sample's lable Y\n",
    "    \n",
    "    # check if (X, Y) is a valid target \n",
    "    pred_y = model.predict(X)\n",
    "    if sum((max(pred_y[0])==pred_y[0]) * Y[0]) == 0:\n",
    "        print(\"not a valid target.\")\n",
    "        invalid_count += 1\n",
    "        continue\n",
    "        \n",
    "\n",
    "    # when performing blackbox attack, we feed forward X3 = [X; X+h; X-h] in the same batch for estimating the gradient\n",
    "    if blackbox:\n",
    "        var = np.repeat(real_adv, batch_size*3, axis=0)\n",
    "        X3 = np.repeat(X, 3, axis=0)\n",
    "        Y3 = np.repeat(Y, 3, axis=0)\n",
    "\n",
    "\n",
    "    # main loop for the optimization\n",
    "    for epoch in tqdm(range(1, MAX_ITE+1)):\n",
    "\n",
    "        if use_train_op: # apply train_op\n",
    "\n",
    "            # whitebox attack\n",
    "\n",
    "            sess.run(train, feed_dict={x:X,y:Y})\n",
    "            adv1, output1, l2dist1, real1, fake1, loss1, new_x1 = sess.run([adv, output, l2dist, real, fake, loss, new_x], feed_dict={x:X,y:Y})\n",
    "            print(l2dist1, adv1, X, new_x1)\n",
    "        else: # apply self-implemented Adam\n",
    "\n",
    "            # estimate gradient. \n",
    "            if use_grad_op: # For fake blackbox attack, just run grad_op.\n",
    "\n",
    "                # fake blackbox attack\n",
    "\n",
    "                true_grads, los, l2s, los1, los2, scores, scores_x, nx, adv1 = sess.run([grad_op, loss, l2dist, loss1, loss2, output, output_x, new_x, adv], feed_dict={adv: real_adv, x:X, y:Y})\n",
    "\n",
    "                # clip the gradient to non-zero coordinates\n",
    "                true_grads[0][0][0:2,sum(abs(X[0][0])>0):]=0\n",
    "\n",
    "                grad = true_grads[0].reshape(-1)\n",
    "\n",
    "            else: # For blackbox attack, apply 1-order discretization\n",
    "\n",
    "                # blackbox attack   \n",
    "\n",
    "                var_size = real_adv[0].size # (2, 506)\n",
    "                # randomly choose a coordinate to compute partial gradient\n",
    "                update_indice = np.random.choice(var_size, 1, replace=True) \n",
    "\n",
    "                # compute coordinate-perturbed input as a batch of size [batch_size*3, 2, 506]\n",
    "                # var = [X; X+h; X-h], X.size = [batch_size, 2, 506]\n",
    "                for i in range(batch_size):\n",
    "                    var[batch_size * 1 + i].reshape(-1)[update_indice[0]] += h\n",
    "                    var[batch_size * 2 + i].reshape(-1)[update_indice[0]] -= h\n",
    "\n",
    "                los, l2s, los_b, scores, nx, adv1 = sess.run([loss, l2dist, loss_batch, output, new_x, adv], feed_dict={adv: var, x:X3, y:Y3})\n",
    "\n",
    "                grad = np.zeros(real_adv.reshape(-1).shape)\n",
    "\n",
    "                # grad(x) = [loss(X+he)-loss(X-he)] / (2h) \n",
    "                for i in range(batch_size):\n",
    "                    grad[update_indice[0]] += los_b[batch_size * 1 + i]- los_b[batch_size * 2 + i]\n",
    "                grad[update_indice[0]] /= 2*h\n",
    "\n",
    "            # Adam update\n",
    "            mt = beta1 * mt + (1 - beta1) * grad\n",
    "            vt = beta2 * vt + (1 - beta2) * np.square(grad)\n",
    "            corr = (math.sqrt(1 - beta2 ** epoch)) / (1 - beta1 ** epoch)\n",
    "\n",
    "            m = real_adv.reshape(-1)\n",
    "            m -= learning_rate * corr * (mt / (np.sqrt(vt) + epsilon))\n",
    "            real_adv = m.reshape(real_adv.shape)\n",
    "\n",
    "            if use_grad_op: \n",
    "    #             print(los1, los2, scores, scores_x)\n",
    "                if epoch == MAX_ITE:\n",
    "                    print(\"attack failed!\")\n",
    "                    fail_count += 1\n",
    "                    break\n",
    "                if sum((scores[0] == max(scores[0]))*Y[0])==0:\n",
    "                    print(\"attack successed! with ite =\", epoch)\n",
    "                    success_count += 1\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print(l2s[[0,batch_size,2*batch_size]], los)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
