{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from utils.generic_utils import load_dataset_at\n",
    "\n",
    "from utils.constants import MAX_NB_VARIABLES, NB_CLASSES_LIST, MAX_TIMESTEPS_LIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading train / test dataset :  ../cut_data/ ../cut_data/\nFinished processing train dataset..\nFinished loading test dataset..\n\nNumber of train samples :  1847 Number of test samples :  793\nNumber of classes :  7\nSequence length :  506\n"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, is_timeseries = load_dataset_at(0, fold_index=None, normalize_timeseries=False) \n",
    "X_test = pad_sequences(X_test, maxlen=MAX_NB_VARIABLES[0], padding='post', truncating='post')\n",
    "Y_test = to_categorical(Y_test, len(np.unique(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(weights = None):\n",
    "    #Squeeze excitation block\n",
    "\n",
    "    def squeeze_excite_block(input):\n",
    "        filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "        se = GlobalAveragePooling1D()(input)\n",
    "        se = Reshape((1, filters))(se)\n",
    "        se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "        se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "        se = multiply([input, se])\n",
    "        return se\n",
    "\n",
    "\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES[0], MAX_TIMESTEPS_LIST[0]))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(8)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(64, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 4, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(64, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASSES_LIST[0], activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=ip, outputs=out)\n",
    "\n",
    "    if(not weights == None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING: Logging before flag parsing goes to stderr.\nW0522 00:07:54.682206 11188 deprecation_wrapper.py:119] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nW0522 00:07:54.709207 11188 deprecation_wrapper.py:119] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nW0522 00:07:54.727198 11188 deprecation_wrapper.py:119] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nW0522 00:07:55.035021 11188 deprecation.py:323] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nW0522 00:07:55.074993 11188 deprecation_wrapper.py:119] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n\nW0522 00:07:55.084989 11188 deprecation.py:506] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nW0522 00:07:55.224894 11188 deprecation_wrapper.py:119] From E:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n\n"
    }
   ],
   "source": [
    "model = get_model('./model_weights931890.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8890290037831021"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#Check for accuracy\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "t = []\n",
    "for i in range(len(Y_pred)):\n",
    "    t.append(sum((max(Y_pred[i])==Y_pred[i]) * Y_test[i]) == 0)\n",
    "1-sum(t)/len(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the time being latent space has the same shape as X_shape\n",
    "latent_vector_shape = (2, 506)\n",
    "X_shape = (2, 506)\n",
    "Y_shape = (7,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE graph\n",
    "sess_autoZoom = tf.InteractiveSession()\n",
    "'''\n",
    "    Construct tf-Graph\n",
    "''' \n",
    "\n",
    "CONST_LAMBDA = 10000\n",
    "k = 0.00\n",
    "x = tf.placeholder(tf.float32, (None,) + X_shape, name='X') #Input data\n",
    "y = tf.placeholder(tf.float32, (None,) + Y_shape, name='Y')    # Output\n",
    "adv = tf.placeholder(tf.float32, (None,) + latent_vector_shape, name='adv') #avdersarial example\n",
    "\n",
    "# compute loss\n",
    "new_x = adv + x\n",
    "output = model(new_x)\n",
    "\n",
    "l2_norm = tf.reduce_sum(tf.square(new_x - x), axis=[1,2])\n",
    "\n",
    "real = tf.reduce_sum(y * output, axis=1)\n",
    "other = tf.reduce_max((1 - y) * output, axis=1)\n",
    "    \n",
    "loss1 = CONST_LAMBDA * tf.maximum(tf.log(real + 1e-30) - tf.log(other + 1e-30), -k)\n",
    "loss2 = l2_norm\n",
    "\n",
    "loss_batch = loss1 + loss2\n",
    "loss = tf.reduce_sum(loss_batch) # sum over all the batch samples\n",
    "\n",
    "# # initialize variables and load target model\n",
    "sess_autoZoom.run(tf.global_variables_initializer())\n",
    "\n",
    "#weights are reset\n",
    "model.load_weights('./model_weights931890.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n start attacking target 0 ...\nattack successed! with iter 1519\n\n start attacking target 1 ...\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-84fc3e2040de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#For estimation of F(x + beta*u) and F(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minit_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_adv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlatent_vector_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess_autoZoom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0madv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[1;31m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[1;31m# of a handle from a different device as an error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m     \u001b[0mfinal_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[0mfinal_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_update_with_movers\u001b[1;34m(self, feed_dict, feed_map)\u001b[0m\n\u001b[0;32m   1403\u001b[0m     \u001b[0mhandle_movers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfeed_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1405\u001b[1;33m       \u001b[0mmover\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_handle_mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1406\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmover\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[0mhandle_movers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmover\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\session_ops.py\u001b[0m in \u001b[0;36m_get_handle_mover\u001b[1;34m(graph, feeder, handle)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_handle_mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m   \u001b[1;34m\"\"\"Return a move subgraph for this pair of feeder and handle.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m   \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_handle_feeder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\session_ops.py\u001b[0m in \u001b[0;36m_get_handle_feeder\u001b[1;34m(graph, feeder)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_handle_feeder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_feeders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeeder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3 2019.03\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mname\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2131\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2132\u001b[0m     \u001b[1;34m\"\"\"The full name of this operation.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2133\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "success_count = 0\n",
    "query_summary = {}\n",
    "adv_summary = {}\n",
    "fail_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "q = 1 \n",
    "b = q\n",
    "#Iterate for each test example\n",
    "for t in range(X_test.shape[0]):\n",
    "\n",
    "    print(\"\\n start attacking target\", t, \"...\")\n",
    "       \n",
    "    mt = 0               # accumulator m_t in Adam\n",
    "    vt = 0               # accumulator v_t in Adam\n",
    "\n",
    "    beta1 = 0.9            # parameter beta_1 in Adam\n",
    "    beta2 = 0.999          # parameter beta_2 in Adam\n",
    "    learning_rate = 2e-3   # learning rate in Adam\n",
    "    \n",
    "    batch_size = 1        # batch size\n",
    "    MAX_ITE = 2000        # maximum number of iterations\n",
    "\n",
    "    #For the time being it has the same shape as X\n",
    "    init_adv = np.zeros((1,) + latent_vector_shape)           # initial adversarial perturbation\n",
    "\n",
    "    X = np.expand_dims(X_test[t], 0)           # target sample X\n",
    "    Y = np.expand_dims(Y_test[t], 0)           # target sample's lable Y\n",
    "    \n",
    "    # check if (X, Y) is a valid target, y checking if it is classified correctly\n",
    "    Y_pred = model.predict(X)\n",
    "    if sum((max(Y_pred[0]) == Y_pred[0]) * Y[0]) == 0:\n",
    "        print(\"not a valid target.\")\n",
    "        invalid_count += 1\n",
    "        continue\n",
    "    \n",
    "    # when performing blackbox attack, we feed forward X = [X + old_adv, X + new_adv] in the same batch for estimating the gradient\n",
    "    X = np.repeat(X, 2, axis=0)\n",
    "    Y = np.repeat(Y, 2, axis=0)\n",
    "\n",
    "    var_size = init_adv.size\n",
    "    beta = 1/(var_size)\n",
    "\n",
    "    # main loop for the optimization\n",
    "    for epoch in range(1, MAX_ITE + 1):\n",
    "        #Using random vector gradient estimation \n",
    "\n",
    "        #random noise\n",
    "        u = np.random.normal(loc=0, scale=1000, size = (q, var_size))\n",
    "        u_mean = np.mean(u, axis=1, keepdims=True)\n",
    "        u_std = np.std(u, axis=1, keepdims=True)\n",
    "        u_norm = np.apply_along_axis(np.linalg.norm, 1, u, keepdims=True)\n",
    "        u = u/u_norm\n",
    "\n",
    "        #For estimation of F(x + beta*u) and F(x)\n",
    "        var = np.concatenate( (init_adv, init_adv + beta * u.reshape((q,)+ (latent_vector_shape)) ), axis=0)\n",
    "        losses, scores = sess_autoZoom.run([loss_batch, output], feed_dict={adv: var, x: X, y: Y}) \n",
    "        \n",
    "\n",
    "        #Gradient estimation\n",
    "        grad = np.zeros((q, var_size), dtype = np.float32)\n",
    "        for j in range(q):\n",
    "            grad[j] = (b * (losses[j + 1] - losses[0])* u[j]) / beta\n",
    "        \n",
    "        avg_grad = np.mean(grad, axis=0)\n",
    "\n",
    "        # ADAM update\n",
    "        mt = beta1 * mt + (1 - beta1) * avg_grad\n",
    "        vt = beta2 * vt + (1 - beta2) * (avg_grad * avg_grad)\n",
    "        corr = (np.sqrt(1 - np.power(beta2,epoch))) / (1 - np.power(beta1, epoch))\n",
    "\n",
    "        m = init_adv .reshape(-1)\n",
    "        m -= learning_rate * corr * (mt / (np.sqrt(vt) + 1e-8))\n",
    "\n",
    "        #update the adversarial example\n",
    "        init_adv = m.reshape(init_adv.shape)\n",
    "\n",
    "\n",
    "        if epoch == MAX_ITE:\n",
    "            print(\"attack failed!\")\n",
    "            fail_count += 1\n",
    "            break\n",
    "\n",
    "        if sum((scores[0] == max(scores[0]))*Y[0])==0:\n",
    "            print(f\"attack successed! with iter {epoch}\")\n",
    "            success_count += 1\n",
    "            query_summary[t] = epoch\n",
    "            adv_summary[t] = init_adv\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "39ec8dfd-498e-4493-baab-2582db0ec534",
   "display_name": "'Python Interactive'"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}